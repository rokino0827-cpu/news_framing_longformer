name: CI - Code Quality and Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black flake8 mypy
        pip install -r requirements.txt
    
    - name: Run Black (code formatting)
      run: |
        black --check --diff .
    
    - name: Run Flake8 (linting)
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Run MyPy (type checking)
      run: |
        mypy --ignore-missing-imports --no-strict-optional data/ models/ training/ eval/

  unit-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov
        pip install -r requirements.txt
    
    - name: Create demo data
      run: |
        python demo/create_demo_data.py
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=. --cov-report=xml --cov-report=term
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  integration-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create demo data
      run: |
        python demo/create_demo_data.py
    
    - name: Validate demo data processing
      run: |
        python demo/validate_demo.py
    
    - name: Test data loading pipeline
      run: |
        python -c "
        from data.dataset import load_sv2000_dataframe, build_labels
        import yaml
        
        # Load demo config
        with open('demo/demo_config.yaml') as f:
            config = yaml.safe_load(f)
        
        # Test data loading
        df = load_sv2000_dataframe(config['data']['train_path'])
        print(f'Loaded {len(df)} samples')
        
        # Test label construction
        y_reg, y_cls, frame_names = build_labels(
            df, 
            config['data']['frame_defs'],
            config['label']['regression_agg'],
            config['label']['normalize'],
            config['label']['presence_threshold']
        )
        print(f'Labels shape: {y_reg.shape}, {y_cls.shape}')
        print('✓ Data pipeline test passed')
        "
    
    - name: Test model forward pass
      run: |
        python -c "
        import torch
        from transformers import LongformerTokenizer
        from models.multitask import LongformerMultiTask
        
        # Test model instantiation and forward pass
        tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
        model = LongformerMultiTask(
            model_name='allenai/longformer-base-4096',
            num_frames=5,
            dropout=0.1
        )
        
        # Test forward pass with dummy data
        batch_size, seq_len = 2, 512
        input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len))
        attention_mask = torch.ones(batch_size, seq_len)
        global_attention_mask = torch.zeros(batch_size, seq_len)
        global_attention_mask[:, 0] = 1  # CLS token
        
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                global_attention_mask=global_attention_mask
            )
        
        print(f'Regression output shape: {outputs[\"regression\"].shape}')
        print(f'Classification output shape: {outputs[\"classification\"].shape}')
        print('✓ Model forward pass test passed')
        "
    
    - name: Test metrics computation
      run: |
        python -c "
        import numpy as np
        from eval.metrics import compute_regression_metrics, compute_presence_metrics, merge_metrics
        
        # Generate dummy data
        n_samples, n_frames = 10, 5
        frame_names = ['conflict', 'human', 'econ', 'moral', 'resp']
        
        y_true_reg = np.random.rand(n_samples, n_frames)
        y_pred_reg = np.random.rand(n_samples, n_frames)
        y_true_cls = (y_true_reg > 0.5).astype(int)
        y_prob_cls = np.random.rand(n_samples, n_frames)
        
        # Test metrics computation
        reg_metrics = compute_regression_metrics(y_true_reg, y_pred_reg, frame_names)
        
        # Test with new threshold strategies format
        threshold_strategies = {
            'balanced': {'strategy': 'per_frame_opt_f1', 'fallback_threshold': 0.5}
        }
        cls_metrics = compute_presence_metrics(y_true_cls, y_prob_cls, frame_names, threshold_strategies)
        
        merged = merge_metrics(reg_metrics, cls_metrics)
        
        print(f'Overall alignment: {merged[\"overall_alignment\"]:.3f}')
        print('✓ Metrics computation test passed')
        "

  reproducibility-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create demo data
      run: |
        python demo/create_demo_data.py
    
    - name: Test reproducible training setup
      run: |
        python scripts/reproduce_training.py --config demo/demo_config.yaml --dry_run
    
    - name: Verify run directory structure
      run: |
        # Check that run directory was created with correct structure
        run_dir=$(ls -1 runs/ | head -1)
        echo "Created run directory: $run_dir"
        
        # Check required subdirectories
        for subdir in checkpoints logs reports config; do
          if [ ! -d "runs/$run_dir/$subdir" ]; then
            echo "Missing directory: runs/$run_dir/$subdir"
            exit 1
          fi
        done
        
        # Check required config files
        for file in system_info.json config.yaml args.json data_info.json; do
          if [ ! -f "runs/$run_dir/config/$file" ]; then
            echo "Missing config file: runs/$run_dir/config/$file"
            exit 1
          fi
        done
        
        echo "✓ Reproducibility setup test passed"